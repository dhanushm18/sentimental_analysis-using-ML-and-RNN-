{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Sentiment Analysis(ML & RNN)",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhanushm18/sentimental_analysis-using-ML-and-RNN-/blob/main/Sentiment_Analysis(ML_%26_RNN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "kazanova_sentiment140_path = kagglehub.dataset_download('kazanova/sentiment140')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "Y3XwE7apseZe"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis Using Various ML Classifiers ans well as using Recurrent Neural Network"
      ],
      "metadata": {
        "id": "5LZtTESrseZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In this notebook you will see text precessing on twitter data set and after that I have performed different Machine Learning Algorithms on the data such as **Logistic Regression, RandomForestClassifier, SVC, Naive Bayes** to classifiy positive and negative tweets. After that I have also built a RNN network which is the best fit for such textual sentiment analysis, since it's a Sequential Dataset which is requirement for RNN network.\n",
        "\n",
        "#### Let's Dive into it."
      ],
      "metadata": {
        "id": "geTqmFoOp0i8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Index\n",
        "\n",
        "* <a href=\"#1\" style=\"color: blue;\">Importing Libraries</a>\n",
        "* <a href=\"#2\" style=\"color: blue;\">Loading Dataset</a>\n",
        "* <a href=\"#3\" style=\"color: blue;\">Data Visualization</a>\n",
        "* <a href=\"#4\" style=\"color: blue;\">Data Preprocessing</a>\n",
        "* <a href=\"#5\" style=\"color: blue;\">Analyzing the Data</a>\n",
        "* <a href=\"#6\" style=\"color: blue;\">Vectorization and Splitting the data</a>\n",
        "* <a href=\"#7\" style=\"color: blue;\">Model Building</a>\n",
        "* <a href=\"#8\" style=\"color: blue;\">Logistic Regression</a>\n",
        "* <a href=\"#9\" style=\"color: blue;\">Linear SVM</a>\n",
        "* <a href=\"#10\" style=\"color: blue;\">Random Forest</a>\n",
        "* <a href=\"#11\" style=\"color: blue;\">Naive Bayes</a>\n",
        "* <a href=\"#12\" style=\"color: blue;\">RNN</a>\n",
        "* <a href=\"#13\" style=\"color: blue;\">Model Saving, Loading and Prediction</a>"
      ],
      "metadata": {
        "id": "h8DY-q4lseZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing libraries <a id=\"1\"></a>"
      ],
      "metadata": {
        "id": "NCfN_akmFrj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrame\n",
        "import pandas as pd\n",
        "\n",
        "# plotting\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# nltk\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "\n",
        "#tensorflow\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Utility\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import re\n",
        "import string\n",
        "import pickle"
      ],
      "metadata": {
        "id": "pVYNu9wMdkko",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Dataset <a id=\"2\"></a>"
      ],
      "metadata": {
        "id": "R7jmsaF2dkkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct a tf.data.Dataset\n",
        "data = pd.read_csv('/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv',encoding='latin', names = ['polarity','id','date','query','user','text'])"
      ],
      "metadata": {
        "id": "hycEYqiDlC8p",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.sample(frac=1)\n",
        "data = data[:200000]"
      ],
      "metadata": {
        "id": "Uu3dZ9eeBZOU",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Visualization <a id=\"3\"></a>"
      ],
      "metadata": {
        "id": "8NMuY7JBwmmy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset details\n",
        "target: the polarity of the tweet (0 = negative, 4 = positive)\n",
        "\n",
        "* date : the date of the tweet (Sat May 16 23:58:44 PDT 2009)\n",
        "* polarity : the polarity of the tweet (0 = negative 4 = positive)\n",
        "* user : the user that tweeted (TerraScene)\n",
        "* text : the text of the tweet (i'm 10x cooler than all of you)"
      ],
      "metadata": {
        "id": "5JSPtS23yPfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset shape:\", data.shape)"
      ],
      "metadata": {
        "id": "f1lt303exS3J",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(10)"
      ],
      "metadata": {
        "id": "MKn0VS_kdkkr",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['polarity'].unique()"
      ],
      "metadata": {
        "id": "CEGPNsXRdkks",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replacing the value 4 -->1 for ease of understanding.\n",
        "data['polarity'] = data['polarity'].replace(4,1)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "Tl4afas1W20w",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "evVpnAuDwo64",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the number of positive vs. negative tagged sentences\n",
        "positives = data['polarity'][data.polarity == 1 ]\n",
        "negatives = data['polarity'][data.polarity == 0 ]\n",
        "\n",
        "print('Total length of the data is:         {}'.format(data.shape[0]))\n",
        "print('No. of positve tagged sentences is:  {}'.format(len(positives)))\n",
        "print('No. of negative tagged sentences is: {}'.format(len(negatives)))"
      ],
      "metadata": {
        "id": "iTN9l7p2w_N0",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get a word count per of text\n",
        "def word_count(words):\n",
        "    return len(words.split())"
      ],
      "metadata": {
        "id": "FZCZYZKn00Iv",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot word count distribution for both positive and negative\n",
        "\n",
        "data['word count'] = data['text'].apply(word_count)\n",
        "p = data['word count'][data.polarity == 1]\n",
        "n = data['word count'][data.polarity == 0]\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.xlim(0,45)\n",
        "plt.xlabel('Word count')\n",
        "plt.ylabel('Frequency')\n",
        "g = plt.hist([p, n], color=['g','r'], alpha=0.5, label=['positive','negative'])\n",
        "plt.legend(loc='upper right')"
      ],
      "metadata": {
        "id": "G25IkW3303gw",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get common words in training dataset\n",
        "from collections import Counter\n",
        "all_words = []\n",
        "for line in list(data['text']):\n",
        "    words = line.split()\n",
        "    for word in words:\n",
        "      if(len(word)>2):\n",
        "        all_words.append(word.lower())\n",
        "\n",
        "\n",
        "Counter(all_words).most_common(20)"
      ],
      "metadata": {
        "id": "AudC8DVW07Cf",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing <a id=\"4\"></a>"
      ],
      "metadata": {
        "id": "XXzTV672dkkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "sns.countplot(data['polarity'])"
      ],
      "metadata": {
        "id": "Da5Dmn9Vdkks",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing the unnecessary columns.\n",
        "data.drop(['date','query','user','word count'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "VE6nqmh5dkku",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop('id', axis=1, inplace=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "gYzN8Zc5seZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(10)"
      ],
      "metadata": {
        "id": "XNSb32D9dkku",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking if any null values present\n",
        "(data.isnull().sum() / len(data))*100"
      ],
      "metadata": {
        "id": "vI1xFqAZdkku",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convrting pandas object to a string type\n",
        "data['text'] = data['text'].astype('str')"
      ],
      "metadata": {
        "id": "ThC3QfwupuUs",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stopword = set(stopwords.words('english'))\n",
        "print(stopword)"
      ],
      "metadata": {
        "id": "sUxte8kadkkt",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "eIbgnD6_p8Dr",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The Preprocessing steps taken are:\n",
        "\n",
        "* Lower Casing: Each text is converted to lowercase.\n",
        "* Removing URLs: Links starting with \"http\" or \"https\" or \"www\" are replaced by \"\".\n",
        "\n",
        "* Removing Usernames: Replace @Usernames with word \"\". (eg: \"@XYZ\" to \"\")\n",
        "* Removing Short Words: Words with length less than 2 are removed.\n",
        "* Removing Stopwords: Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. (eg: \"the\", \"he\", \"have\")\n",
        "* Lemmatizing: Lemmatization is the process of converting a word to its base form. (e.g: “wolves” to “wolf”)"
      ],
      "metadata": {
        "id": "wgKLmakU4bTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urlPattern = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
        "userPattern = '@[^\\s]+'\n",
        "def process_tweets(tweet):\n",
        "  # Lower Casing\n",
        "    tweet = tweet.lower()\n",
        "    tweet=tweet[1:]\n",
        "    # Removing all URls\n",
        "    tweet = re.sub(urlPattern,'',tweet)\n",
        "    # Removing all @username.\n",
        "    tweet = re.sub(userPattern,'', tweet)\n",
        "    #Remove punctuations\n",
        "    tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
        "    #tokenizing words\n",
        "    tokens = word_tokenize(tweet)\n",
        "    #Removing Stop Words\n",
        "    final_tokens = [w for w in tokens if w not in stopword]\n",
        "    #reducing a word to its word stem\n",
        "    wordLemm = WordNetLemmatizer()\n",
        "    finalwords=[]\n",
        "    for w in final_tokens:\n",
        "      if len(w)>1:\n",
        "        word = wordLemm.lemmatize(w)\n",
        "        finalwords.append(word)\n",
        "    return ' '.join(finalwords)"
      ],
      "metadata": {
        "id": "1qK7KlBpdkkt",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['processed_tweets'] = data['text'].apply(lambda x: process_tweets(x))\n",
        "print('Text Preprocessing complete.')"
      ],
      "metadata": {
        "id": "We9fsGPzdkku",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(10)"
      ],
      "metadata": {
        "id": "25A79MOtdkkv",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyzing the data <a id=\"5\"></a>\n",
        "\n",
        "> #### Now we're going to analyse the preprocessed data to get an understanding of it. We'll plot Word Clouds for Positive and Negative tweets from our dataset and see which words occur the most."
      ],
      "metadata": {
        "id": "8oYwlcqgqXZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word-Cloud for Negative tweets."
      ],
      "metadata": {
        "id": "vu7ZqLw3rRWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (15,15))\n",
        "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(data[data.polarity == 0].processed_tweets))\n",
        "plt.imshow(wc , interpolation = 'bilinear')"
      ],
      "metadata": {
        "id": "M5VSi3yitet8",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word-Cloud for Positive tweets."
      ],
      "metadata": {
        "id": "rgAuqnsLrXPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (15,15))\n",
        "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(data[data.polarity == 1].processed_tweets))\n",
        "plt.imshow(wc , interpolation = 'bilinear')"
      ],
      "metadata": {
        "id": "43faZ5-JuDBa",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorization and Splitting the data <a id=\"6\"></a>\n",
        "Storing input variable-processes_tweets to X and output variable-polarity to y"
      ],
      "metadata": {
        "id": "YqARyAu8Ad1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data['processed_tweets'].values\n",
        "y = data['polarity'].values\n"
      ],
      "metadata": {
        "id": "F_H_S5Xgdkkv",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "V7jVqik-dkkv",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert text to word frequency vectors\n",
        "### TF-IDF\n",
        " This is an acronym than stands for **Term Frequency – Inverse Document** Frequency which are the components of the resulting scores assigned to each word.\n",
        "\n",
        "* Term Frequency: This summarizes how often a given word appears within a document.\n",
        "* Inverse Document Frequency: This downscales words that appear a lot across documents."
      ],
      "metadata": {
        "id": "mZM6jsR8_cJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert a collection of raw documents to a matrix of TF-IDF features.\n",
        "vector = TfidfVectorizer(sublinear_tf=True)\n",
        "X = vector.fit_transform(X)\n",
        "print(f'Vector fitted.')\n",
        "print('No. of feature_words: ', len(vector.get_feature_names()))"
      ],
      "metadata": {
        "id": "jFJC-pg8dkkw",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "K7OeXCrQdkkw",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split train and test"
      ],
      "metadata": {
        "id": "2h8-AkgrBJC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Preprocessed Data is divided into 2 sets of data:\n",
        "\n",
        "* Training Data: The dataset upon which the model would be trained on. Contains 80% data.\n",
        "* Test Data: The dataset upon which the model would be tested against. Contains 20% data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O7uscJT049G3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=101)"
      ],
      "metadata": {
        "id": "RgFgy_iBdkkw",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train\", X_train.shape)\n",
        "print(\"y_train\", y_train.shape)\n",
        "print()\n",
        "print(\"X_test\", X_test.shape)\n",
        "print(\"y_test\", y_test.shape)"
      ],
      "metadata": {
        "id": "7h_jfcqedkkw",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Building <a id=\"7\"></a>"
      ],
      "metadata": {
        "id": "bk6zPzyxsPa-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model evaluating function"
      ],
      "metadata": {
        "id": "b4hgNd80Gqv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_Evaluate(model):\n",
        "    #accuracy of model on training data\n",
        "    acc_train=model.score(X_train, y_train)\n",
        "    #accuracy of model on test data\n",
        "    acc_test=model.score(X_test, y_test)\n",
        "\n",
        "    print('Accuracy of model on training data : {}'.format(acc_train*100))\n",
        "    print('Accuracy of model on testing data : {} \\n'.format(acc_test*100))\n",
        "\n",
        "    # Predict values for Test dataset\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Print the evaluation metrics for the dataset.\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Compute and plot the Confusion matrix\n",
        "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    categories  = ['Negative','Positive']\n",
        "    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n",
        "    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
        "\n",
        "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "\n",
        "    sns.heatmap(cf_matrix, annot = labels, cmap = 'Reds',fmt = '',\n",
        "                xticklabels = categories, yticklabels = categories)\n",
        "\n",
        "    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n",
        "    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n",
        "    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)"
      ],
      "metadata": {
        "id": "aTM2kj98C9xq",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression <a id=\"8\"></a>"
      ],
      "metadata": {
        "id": "Mn-LI-2oJFjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lg = LogisticRegression()\n",
        "history=lg.fit(X_train, y_train)\n",
        "model_Evaluate(lg)"
      ],
      "metadata": {
        "id": "KJ8RdPM6dkkx",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear SVM <a id=\"9\"></a>"
      ],
      "metadata": {
        "id": "oUwXXr1nI_rn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm = LinearSVC()\n",
        "svm.fit(X_train, y_train)\n",
        "model_Evaluate(svm)"
      ],
      "metadata": {
        "id": "H6hOCx_pqc6m",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest <a id=\"10\"></a>"
      ],
      "metadata": {
        "id": "zrRHaqy_I5qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(n_estimators = 20, criterion = 'entropy', max_depth=50)\n",
        "rf.fit(X_train, y_train)\n",
        "model_Evaluate(rf)"
      ],
      "metadata": {
        "id": "yYnt9mRTHRbb",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Naive Bayes <a id=\"11\"></a>"
      ],
      "metadata": {
        "id": "JCvxpxFMI2JW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb = BernoulliNB()\n",
        "nb.fit(X_train, y_train)\n",
        "model_Evaluate(nb)"
      ],
      "metadata": {
        "id": "NSwMQQGRI0fp",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN <a id=\"12\"></a>"
      ],
      "metadata": {
        "id": "d07tHN83wOUz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What is RNN?\n",
        "Recurrent neural networks (RNN) are the state of the art algorithm for sequential data and are used by Apple's Siri and and Google's voice search. It is the first algorithm that remembers its input, due to an internal memory, which makes it perfectly suited for machine learning problems that involve sequential data\n",
        "\n",
        "#### Embedding Layer\n",
        "Embedding layer is one of the available layers in Keras. This is mainly used in Natural Language Processing related applications such as language modeling, but it can also be used with other tasks that involve neural networks. While dealing with NLP problems, we can use pre-trained word embeddings such as GloVe. Alternatively we can also train our own embeddings using Keras embedding layer.\n",
        "\n",
        "#### LSTM layer\n",
        "Long Short Term Memory networks, usually called “LSTMs” , were introduced by Hochreiter and Schmiduber. These have widely been used for speech recognition, language modeling, sentiment analysis and text prediction. Before going deep into LSTM, we should first understand the need of LSTM which can be explained by the drawback of practical use of Recurrent Neural Network (RNN). So, lets start with RNN."
      ],
      "metadata": {
        "id": "6fAJBzpEefLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import re"
      ],
      "metadata": {
        "id": "z-O59gYEyV7B",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "metadata": {
        "id": "_HpLbl0z-kZg",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import regularizers\n",
        "\n",
        "max_words = 5000\n",
        "max_len = 200\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(data.processed_tweets)\n",
        "sequences = tokenizer.texts_to_sequences(data.processed_tweets)\n",
        "tweets = pad_sequences(sequences, maxlen=max_len)\n",
        "print(tweets)"
      ],
      "metadata": {
        "id": "68irxgj85Duv",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(tweets, data.polarity.values, test_size=0.2, random_state=101)"
      ],
      "metadata": {
        "id": "GVXtFMC453Hi",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "model2 = Sequential()\n",
        "model2.add(layers.Embedding(max_words, 128))\n",
        "model2.add(layers.LSTM(64,dropout=0.5))\n",
        "model2.add(layers.Dense(16, activation='relu'))\n",
        "model2.add(layers.Dense(8, activation='relu'))\n",
        "model2.add(layers.Dense(1,activation='sigmoid'))\n",
        "model2.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n",
        "checkpoint2 = ModelCheckpoint(\"rnn_model.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\n",
        "history = model2.fit(X_train, y_train, epochs=10,validation_data=(X_test, y_test),callbacks=[checkpoint2])\n"
      ],
      "metadata": {
        "id": "eGpuqkUj5Dxv",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = tokenizer.texts_to_sequences(['this data science article is the worst ever'])\n",
        "test = pad_sequences(sequence, maxlen=max_len)\n",
        "pred = model2.predict(test)\n",
        "if pred > 0.5:\n",
        "  print('Positive')\n",
        "else:\n",
        "  print('Negative')\n",
        "# print(pred)"
      ],
      "metadata": {
        "id": "NjU0Kf7idfJf",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model('rnn_model.hdf5')\n",
        "sequence = tokenizer.texts_to_sequences(['this data science article is the best ever'])\n",
        "test = pad_sequences(sequence, maxlen=max_len)\n",
        "pred = model.predict(test)\n",
        "if pred > 0.5:\n",
        "  print('Positive')\n",
        "else:\n",
        "  print('Negative')"
      ],
      "metadata": {
        "id": "HJXJWC53PGEp",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = tokenizer.texts_to_sequences(['I had a bad day at work.'])\n",
        "test = pad_sequences(sequence, maxlen=max_len)\n",
        "pred = model.predict(test)\n",
        "if pred > 0.5:\n",
        "  print('Positive')\n",
        "else:\n",
        "  print('Negative')"
      ],
      "metadata": {
        "id": "_Bw4rZQjTWEw",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Saving, Loading and Prediction <a id=\"13\"></a>"
      ],
      "metadata": {
        "id": "S4hTkNceDOqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "file = open('vectoriser.pickle','wb')\n",
        "pickle.dump(vector, file)\n",
        "file.close()\n",
        "\n",
        "file = open('logisticRegression.pickle','wb')\n",
        "pickle.dump(lg, file)\n",
        "file.close()\n",
        "\n",
        "file = open('SVM.pickle','wb')\n",
        "pickle.dump(svm, file)\n",
        "file.close()\n",
        "\n",
        "file = open('RandomForest.pickle','wb')\n",
        "pickle.dump(rf, file)\n",
        "file.close()\n",
        "\n",
        "file = open('NaivesBayes.pickle','wb')\n",
        "pickle.dump(nb, file)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "8yCr6DDCOs0g",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict using saved model"
      ],
      "metadata": {
        "id": "4GOWIRETDwCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_models():\n",
        "    # Load the vectoriser.\n",
        "    file = open('vectoriser.pickle', 'rb')\n",
        "    vectoriser = pickle.load(file)\n",
        "    file.close()\n",
        "    # Load the LR Model.\n",
        "    file = open('logisticRegression.pickle', 'rb')\n",
        "    lg = pickle.load(file)\n",
        "    file.close()\n",
        "    return vectoriser, lg"
      ],
      "metadata": {
        "id": "nHrQifAQDCyc",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(vectoriser, model, text):\n",
        "    # Predict the sentiment\n",
        "    processes_text=[process_tweets(sen) for sen in text]\n",
        "    textdata = vectoriser.transform(processes_text)\n",
        "    sentiment = model.predict(textdata)\n",
        "\n",
        "    # Make a list of text with sentiment.\n",
        "    data = []\n",
        "    for text, pred in zip(text, sentiment):\n",
        "        data.append((text,pred))\n",
        "    # Convert the list into a Pandas DataFrame.\n",
        "    df = pd.DataFrame(data, columns = ['text','sentiment'])\n",
        "    df = df.replace([0,1], [\"Negative\",\"Positive\"])\n",
        "    return df"
      ],
      "metadata": {
        "id": "VAqUYn4MDHsB",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__==\"__main__\":\n",
        "    # Loading the models.\n",
        "    vectoriser, lg = load_models()\n",
        "\n",
        "    # Text to classify should be in a list.\n",
        "    text = [\"I love machine learning\",\n",
        "            \"Work is too hectic.\",\n",
        "            \"Mr.Sharama, I feel so good\"]\n",
        "\n",
        "    df = predict(vectoriser, lg, text)\n",
        "    print(df.head())\n"
      ],
      "metadata": {
        "id": "CEdCqF0oW7em",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bq0PBtDTseZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ZOK6eVfseZ5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}